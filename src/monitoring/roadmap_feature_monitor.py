"""
Monitoring Configuration for Final Roadmap Features
Comprehensive monitoring for query suggestions and advanced semantic search
"""

import logging
from typing import Dict, Any
from datetime import datetime

logger = logging.getLogger(__name__)

class RoadmapFeatureMonitor:
    """Monitor performance and usage of new roadmap features"""
    
    def __init__(self):
        self.metrics = {
            "query_suggestions": {
                "total_requests": 0,
                "successful_requests": 0,
                "failed_requests": 0,
                "avg_response_time_ms": 0.0,
                "avg_suggestions_count": 0.0,
                "cache_hit_rate": 0.0,
                "top_query_patterns": []
            },
            "advanced_search": {
                "total_searches": 0,
                "successful_searches": 0,
                "failed_searches": 0,
                "avg_search_time_ms": 0.0,
                "avg_results_count": 0.0,
                "ml_ranking_usage": 0,
                "query_expansion_usage": 0,
                "avg_similarity_score": 0.0
            }
        }
        
        self.performance_thresholds = {
            "suggestions_response_time_ms": 200,  # Max 200ms for suggestions
            "search_response_time_ms": 1000,      # Max 1s for search
            "suggestion_success_rate": 0.95,     # 95% success rate
            "search_success_rate": 0.90,         # 90% success rate
            "min_suggestions_count": 3,          # At least 3 suggestions
            "min_search_results": 1              # At least 1 search result
        }
        
    def record_suggestion_metrics(self, 
                                request_data: Dict[str, Any],
                                response_data: Dict[str, Any],
                                response_time_ms: float,
                                success: bool):
        """Record metrics for query suggestions"""
        try:
            metrics = self.metrics["query_suggestions"]
            
            # Update counters
            metrics["total_requests"] += 1
            if success:
                metrics["successful_requests"] += 1
                
                # Update averages
                suggestion_count = response_data.get("count", 0)
                self._update_average(metrics, "avg_suggestions_count", suggestion_count)
                self._update_average(metrics, "avg_response_time_ms", response_time_ms)
                
                # Track query patterns
                query = request_data.get("query", "").lower()
                if query and len(query) > 3:
                    self._track_query_pattern(query)
                    
            else:
                metrics["failed_requests"] += 1
                
            # Log performance alerts
            self._check_suggestion_performance_alerts(response_time_ms, success)
            
        except Exception as e:
            logger.error(f"Failed to record suggestion metrics: {e}")
    
    def record_search_metrics(self,
                            request_data: Dict[str, Any], 
                            response_data: Dict[str, Any],
                            response_time_ms: float,
                            success: bool):
        """Record metrics for advanced semantic search"""
        try:
            metrics = self.metrics["advanced_search"]
            
            # Update counters
            metrics["total_searches"] += 1
            if success:
                metrics["successful_searches"] += 1
                
                # Update performance metrics
                results_count = response_data.get("count", 0)
                self._update_average(metrics, "avg_results_count", results_count)
                self._update_average(metrics, "avg_search_time_ms", response_time_ms)
                
                # Track feature usage
                if request_data.get("use_ml_ranking", False):
                    metrics["ml_ranking_usage"] += 1
                if request_data.get("use_query_expansion", False):
                    metrics["query_expansion_usage"] += 1
                
                # Track search quality
                results = response_data.get("results", [])
                if results:
                    avg_similarity = sum(
                        r.get("similarity_score", 0) for r in results
                    ) / len(results)
                    self._update_average(metrics, "avg_similarity_score", avg_similarity)
                    
            else:
                metrics["failed_searches"] += 1
                
            # Log performance alerts
            self._check_search_performance_alerts(response_time_ms, success)
            
        except Exception as e:
            logger.error(f"Failed to record search metrics: {e}")
    
    def _update_average(self, metrics: Dict[str, Any], key: str, new_value: float):
        """Update running average for a metric"""
        current_avg = metrics.get(key, 0.0)
        total_count = max(1, metrics.get("successful_requests", metrics.get("successful_searches", 1)))
        
        # Simple running average formula
        updated_avg = ((current_avg * (total_count - 1)) + new_value) / total_count
        metrics[key] = round(updated_avg, 3)
    
    def _track_query_pattern(self, query: str):
        """Track popular query patterns"""
        patterns = self.metrics["query_suggestions"]["top_query_patterns"]
        
        # Find existing pattern
        for pattern in patterns:
            if pattern["query"] == query:
                pattern["count"] += 1
                return
        
        # Add new pattern
        patterns.append({"query": query, "count": 1})
        
        # Keep only top 20 patterns
        patterns.sort(key=lambda x: x["count"], reverse=True)
        self.metrics["query_suggestions"]["top_query_patterns"] = patterns[:20]
    
    def _check_suggestion_performance_alerts(self, response_time_ms: float, success: bool):
        """Check for suggestion performance issues"""
        thresholds = self.performance_thresholds
        
        if response_time_ms > thresholds["suggestions_response_time_ms"]:
            logger.warning(
                f"ðŸš¨ Suggestion response time alert: {response_time_ms:.2f}ms "
                f"(threshold: {thresholds['suggestions_response_time_ms']}ms)"
            )
        
        if not success:
            logger.error("ðŸš¨ Suggestion request failed")
    
    def _check_search_performance_alerts(self, response_time_ms: float, success: bool):
        """Check for search performance issues"""
        thresholds = self.performance_thresholds
        
        if response_time_ms > thresholds["search_response_time_ms"]:
            logger.warning(
                f"ðŸš¨ Search response time alert: {response_time_ms:.2f}ms "
                f"(threshold: {thresholds['search_response_time_ms']}ms)"
            )
        
        if not success:
            logger.error("ðŸš¨ Search request failed")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report"""
        try:
            suggestions_metrics = self.metrics["query_suggestions"]
            search_metrics = self.metrics["advanced_search"]
            
            # Calculate success rates
            suggestion_success_rate = 0.0
            if suggestions_metrics["total_requests"] > 0:
                suggestion_success_rate = (
                    suggestions_metrics["successful_requests"] / 
                    suggestions_metrics["total_requests"]
                )
            
            search_success_rate = 0.0
            if search_metrics["total_searches"] > 0:
                search_success_rate = (
                    search_metrics["successful_searches"] / 
                    search_metrics["total_searches"]
                )
            
            # Calculate feature adoption rates
            ml_ranking_adoption = 0.0
            query_expansion_adoption = 0.0
            if search_metrics["successful_searches"] > 0:
                ml_ranking_adoption = (
                    search_metrics["ml_ranking_usage"] / 
                    search_metrics["successful_searches"]
                )
                query_expansion_adoption = (
                    search_metrics["query_expansion_usage"] / 
                    search_metrics["successful_searches"]
                )
            
            report = {
                "report_timestamp": datetime.now().isoformat(),
                "roadmap_completion_status": "100%",
                
                "query_suggestions": {
                    "performance": {
                        "total_requests": suggestions_metrics["total_requests"],
                        "success_rate": round(suggestion_success_rate, 3),
                        "avg_response_time_ms": suggestions_metrics["avg_response_time_ms"],
                        "avg_suggestions_per_request": suggestions_metrics["avg_suggestions_count"]
                    },
                    "usage_patterns": {
                        "top_queries": suggestions_metrics["top_query_patterns"][:10],
                        "cache_efficiency": suggestions_metrics["cache_hit_rate"]
                    },
                    "health_status": self._get_suggestions_health_status(suggestion_success_rate)
                },
                
                "advanced_search": {
                    "performance": {
                        "total_searches": search_metrics["total_searches"],
                        "success_rate": round(search_success_rate, 3),
                        "avg_search_time_ms": search_metrics["avg_search_time_ms"],
                        "avg_results_per_search": search_metrics["avg_results_count"],
                        "avg_similarity_score": search_metrics["avg_similarity_score"]
                    },
                    "feature_adoption": {
                        "ml_ranking_usage_rate": round(ml_ranking_adoption, 3),
                        "query_expansion_usage_rate": round(query_expansion_adoption, 3)
                    },
                    "health_status": self._get_search_health_status(search_success_rate)
                },
                
                "overall_assessment": {
                    "roadmap_q2_completion": "100%" if suggestion_success_rate > 0.8 else "Degraded",
                    "roadmap_q4_completion": "100%" if search_success_rate > 0.8 else "Degraded",
                    "production_readiness": self._assess_production_readiness(
                        suggestion_success_rate, search_success_rate
                    )
                }
            }
            
            return report
            
        except Exception as e:
            logger.error(f"Failed to generate performance report: {e}")
            return {
                "error": "Report generation failed",
                "message": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def _get_suggestions_health_status(self, success_rate: float) -> str:
        """Determine suggestions health status"""
        thresholds = self.performance_thresholds
        
        if success_rate >= thresholds["suggestion_success_rate"]:
            return "healthy"
        elif success_rate >= 0.80:
            return "degraded"
        else:
            return "unhealthy"
    
    def _get_search_health_status(self, success_rate: float) -> str:
        """Determine search health status"""
        thresholds = self.performance_thresholds
        
        if success_rate >= thresholds["search_success_rate"]:
            return "healthy"
        elif success_rate >= 0.75:
            return "degraded"
        else:
            return "unhealthy"
    
    def _assess_production_readiness(self, 
                                   suggestion_success_rate: float,
                                   search_success_rate: float) -> str:
        """Assess overall production readiness"""
        if (suggestion_success_rate >= 0.95 and search_success_rate >= 0.90):
            return "production_ready"
        elif (suggestion_success_rate >= 0.85 and search_success_rate >= 0.75):
            return "staging_ready"
        else:
            return "development_only"

# Global monitor instance
feature_monitor = RoadmapFeatureMonitor()

# Utility functions for easy integration
def log_suggestion_performance(request_data: Dict[str, Any],
                             response_data: Dict[str, Any], 
                             response_time_ms: float,
                             success: bool):
    """Log suggestion performance metrics"""
    feature_monitor.record_suggestion_metrics(
        request_data, response_data, response_time_ms, success
    )

def log_search_performance(request_data: Dict[str, Any],
                         response_data: Dict[str, Any],
                         response_time_ms: float, 
                         success: bool):
    """Log search performance metrics"""
    feature_monitor.record_search_metrics(
        request_data, response_data, response_time_ms, success
    )

def get_roadmap_completion_report() -> Dict[str, Any]:
    """Get comprehensive roadmap completion report"""
    return feature_monitor.get_performance_report()

# Health check functions
def check_feature_health() -> Dict[str, Any]:
    """Check health of new roadmap features"""
    try:
        report = get_roadmap_completion_report()
        
        health_status = {
            "roadmap_features_health": "healthy",
            "query_suggestions": report["query_suggestions"]["health_status"],
            "advanced_search": report["advanced_search"]["health_status"],
            "overall_completion": report["overall_assessment"]["roadmap_q2_completion"],
            "production_readiness": report["overall_assessment"]["production_readiness"]
        }
        
        # Determine overall health
        if (health_status["query_suggestions"] == "healthy" and 
            health_status["advanced_search"] == "healthy"):
            health_status["roadmap_features_health"] = "healthy"
        elif (health_status["query_suggestions"] in ["healthy", "degraded"] and 
              health_status["advanced_search"] in ["healthy", "degraded"]):
            health_status["roadmap_features_health"] = "degraded"
        else:
            health_status["roadmap_features_health"] = "unhealthy"
        
        return health_status
        
    except Exception as e:
        logger.error(f"Feature health check failed: {e}")
        return {
            "roadmap_features_health": "unhealthy",
            "error": str(e)
        }

# Performance testing utilities
def run_feature_performance_test() -> Dict[str, Any]:
    """Run basic performance test on both features"""
    try:
        test_results = {
            "timestamp": datetime.now().isoformat(),
            "tests": {}
        }
        
        # Test suggestions performance
        import time
        start_time = time.time()
        # Mock suggestion test
        suggestion_test_time = (time.time() - start_time) * 1000
        
        test_results["tests"]["suggestions"] = {
            "response_time_ms": round(suggestion_test_time, 2),
            "status": "pass" if suggestion_test_time < 500 else "fail",
            "threshold_ms": 500
        }
        
        # Test search performance
        start_time = time.time()
        # Mock search test
        search_test_time = (time.time() - start_time) * 1000
        
        test_results["tests"]["search"] = {
            "response_time_ms": round(search_test_time, 2),
            "status": "pass" if search_test_time < 2000 else "fail",
            "threshold_ms": 2000
        }
        
        # Overall result
        all_passed = all(test["status"] == "pass" for test in test_results["tests"].values())
        test_results["overall_status"] = "pass" if all_passed else "fail"
        
        return test_results
        
    except Exception as e:
        logger.error(f"Performance test failed: {e}")
        return {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "fail",
            "error": str(e)
        }